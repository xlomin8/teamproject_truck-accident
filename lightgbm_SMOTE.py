#!/usr/bin/env python
# coding: utf-8


import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib
import matplotlib.font_manager as fm
import matplotlib.patheffects as path_effects
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, confusion_matrix
from sklearn.metrics import f1_score


# In[2]:


# í•œê¸€ ê¹¨ì§ ë°©ì§€
import matplotlib
import matplotlib.font_manager as fm
# fm._rebuild()
fm.get_fontconfig_fonts()
font_location = 'C:/Users/ASIA-18/NanumGothic.ttf' # í°íŠ¸ íŒŒì¼ ì´ë¦„, ë””ë ‰í† ë¦¬ ì£¼ì˜
font_name = fm.FontProperties(fname=font_location).get_name()
matplotlib.rc('font', family=font_name)

# ë°ì´í„° ë¡œë“œ
df = pd.read_excel('./cleandata_final_0624.xlsx')
# print(df.columns)



# ë…ë¦½ë³€ìˆ˜(íŠ¹ì§•), ì¢…ì†ë³€ìˆ˜(ë¼ë²¨) ë‚˜ëˆ„ê¸°
X = df.drop(['ì‚¬ë§', 'ë¶€ìƒììˆ˜', 'êµí†µì‚¬ê³ ë¹„ìš©'], axis=1)
Y = df['ì‚¬ë§']

# í›ˆë ¨ì…‹, ê²€ì¦ì…‹ ë‚˜ëˆ„ê¸°
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2,  stratify=Y)

# ë°ì´í„° ë¶ˆê· í˜• í™•ì¸
print(Y_train.value_counts())
# 0 1919
# 1 171

# ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚°
print(Y_train.value_counts().iloc[0] / Y_train.value_counts().iloc[-1])
# 11.2222222222..

## SMOTE
from imblearn.over_sampling import SMOTE
sm = SMOTE(k_neighbors=3)
X_train_sm, Y_train_sm = sm.fit_resample(X_train, Y_train)
X_train_sm = pd.DataFrame(X_train_sm, columns=X.columns)
Y_train_sm = pd.Series(Y_train_sm)

# LightGBM
# ì¥ì )
# - í•™ìŠµí•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì´ ì ë‹¤, ë¹ ë¥¸ ì†ë„
# - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ìƒëŒ€ì ìœ¼ë¡œ ì ì€í¸ì´ë‹¤
# - categorical featureë“¤ì˜ ìë™ ë³€í™˜ê³¼ ìµœì  ë¶„í• 
# - GPU í•™ìŠµ ì§€ì›
# ë‹¨ì )
# - ì‘ì€ datasetì„ ì‚¬ìš©í•  ê²½ìš° ê³¼ì í•© ê°€ëŠ¥ì„±ì´ í¬ë‹¤ (ì¼ë°˜ì ìœ¼ë¡œ 10,000ê°œ ì´í•˜ì˜ ë°ì´í„°ë¥¼ ì ë‹¤ê³  í•œë‹¤)
# ì¶œì²˜: https://mac-user-guide.tistory.com/79 [ğŸŒ·ë‚˜ì˜ ì„ ì¸ì¥ğŸŒµ:í‹°ìŠ¤í† ë¦¬]
# !pip install lightgbm


# #ë°ì´í„°ë¥¼ í‘œì¤€í™” ì‹œí‚´
# from sklearn.preprocessing import StandardScaler


# #í›ˆë ¨ìš© ë¿ë§Œ ì•„ë‹ˆë¼ í…ŒìŠ¤íŠ¸ìš©ë„ ê°™ì´ 

# ss = StandardScaler()
# ss.fit(X_train)
# train_scaled = ss.transform(X_train)
# test_scaled = ss.transform(X_test)


# ëª¨ë¸ë§
import lightgbm as lgb

d_train = lgb.Dataset (X_train_sm, label = Y_train_sm)
params = {}
params [ 'learning_rate'] = 0.02
params [ 'boosting_type'] = 'gbdt' # GradientBoostingDecisionTree
params ['objective'] = 'binary'
params ['boost_from_average'] = False
params [ 'metric' ] = 'binary_logloss' # metric for binary-class
params [ 'max_depth'] = 2
params [ 'num_leaves' ] = 4 # ìµœëŒ€ leavesëŠ” 2^(max_depth)
params ['seed'] = 23456


# í•™ìŠµ
clf = lgb.train (params, d_train, 500) # 1000 epocsì—ì„œ ëª¨ë¸ í›ˆë ¨

# ê²€ì¦
from sklearn.metrics import classification_report

Y_pred_train_sm = clf.predict(X_train_sm)
for i in range(0,len(Y_pred_train_sm)):
    if Y_pred_train_sm[i]>=.3:       # setting threshold to .5
       Y_pred_train_sm[i]=1
    else:
       Y_pred_train_sm[i]=0

Y_pred_test = clf.predict(X_test)
for i in range(0,len(Y_pred_test)):
    if Y_pred_test[i]>=.3:       # setting threshold to .5
       Y_pred_test[i]=1
    else:
       Y_pred_test[i]=0

print(classification_report(Y_train_sm, Y_pred_train_sm))
print(classification_report(Y_test, Y_pred_test))


print(pd.Series(Y_pred_test).value_counts())


# ê³¼ì í•© ë¬¸ì œ, Trainê³¼ Test setì— ì„±ëŠ¥ì„ ìµœëŒ€í•œ ì¤„ì—¬ì£¼ëŠ” ê²ƒì´ ê³¼ì í•©ì„ ë°©ì§€
from sklearn.metrics import roc_auc_score

Y_pred_train_sm_proba = clf.predict(X_train_sm)
Y_pred_test_proba = clf.predict(X_test)


roc_score_train = roc_auc_score(Y_train_sm, Y_pred_train_sm_proba)
roc_score_test = roc_auc_score(Y_test, Y_pred_test_proba)

print("roc_score_train :", roc_score_train)
print("roc_score_test :", roc_score_test)



from sklearn.metrics import roc_curve

def roc_curve_plot(Y_test, pred_proba_c1):
    # ì„ê³—ê°’ì— ë”°ë¥¸ FPR, TPR ê°’ì„ ë°˜í™˜ ë°›ìŒ.
    # FPR : ì•”í™˜ìê°€ ì•„ë‹Œ í™˜ìë¥¼ ì•”í™˜ìë¼ê³  ì˜ ëª» ì˜ˆì¸¡í•œ ë¹„ìœ¨
    # TPR : Recall
    fprs, tprs, thresholds = roc_curve(Y_test, pred_proba_c1)

    # ROC Curveë¥¼ plot ê³¡ì„ ìœ¼ë¡œ ê·¸ë¦¼.
    plt.plot(fprs, tprs, label='ROC')
    # ê°€ìš´ë° ëŒ€ê°ì„  ì§ì„ ì„ ê·¸ë¦¼.
    plt.plot([0, 1], [0, 1], 'k--', label='Random', color='red')

    # FPR X ì¶•ì˜ Scaleì„ 0.1 ë‹¨ìœ„ë¡œ ë³€ê²½, X,Y ì¶•ëª… ì„¤ì •ë“±
    start, end = plt.xlim()
    plt.xticks(np.round(np.arange(start, end, 0.1), 2))
    plt.xlim(0, 1)
    plt.ylim(0, 1)
    plt.xlabel('FPR( 1 - SensitivitY )')
    plt.ylabel('TPR( Recall )')
    plt.legend()
    plt.show()

roc_curve_plot(Y_train_sm, Y_pred_train_sm_proba)

roc_curve_plot(Y_test, Y_pred_test_proba)



import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
plt.style.use(['dark_background'])

ftr_importances_values = clf.feature_importance() # Randomforest : feature_importance_
ftr_importances = pd.Series(ftr_importances_values, index = X.columns)
ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]

plt.figure(figsize=(8,6))
plt.title('Feature Importances')
sns.barplot(x=ftr_top20, y=ftr_top20.index)
plt.show()

